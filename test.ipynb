{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Transcribe \n",
    "* Langchain\n",
    "* LangSmith\n",
    "* Semantic Kernel\n",
    "* Async Calls\n",
    "* Build APIs\n",
    "* Integrate Airflow, DB, Logging\n",
    "\n",
    "Models:\n",
    "* Mistral Dolphin: \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load PIG Code (optionsal: Upload sample data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deepgram.com/learn/local-llm-chatbot-that-can-run-code-searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.1. LLM transcriber PIG2PySpark\n",
    "# Prompt: \n",
    "# You are an experienced Software Engineer and Machine Learning Engineer fluent in PIG and PySpark coding languages. \n",
    "# Rewrite the following PIG code into PySpark so that they can perform identical tasks and output identical results given same input data. \n",
    "# PIG Code: {pig_code}\n",
    "# A.2. Code Parser: Parse and save PySpark code \n",
    "\n",
    "# B. Sample Data Builder \n",
    "# C. PIG Interpreter \n",
    "# D. PySpark Interpreter \n",
    "\n",
    "# If sample data NOT available (run B.)\n",
    "  # Run PIG code and generate output \n",
    "  # 1. Generate PySpark code from PIG (run A.1) and parse/save the PySpark Code (run A.2)\n",
    "# 2. Run against sample data in a separate module.\n",
    "# 3. Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred during Pig script execution:\n",
      "2024-05-02 23:05:26,996 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n",
      "2024-05-02 23:05:26,996 [main] INFO  org.apache.pig.Main - Logging error messages to: /workspace/pig_1714691126990.log\n",
      "2024-05-02 23:05:27,015 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2024-05-02 23:05:27,261 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2024-05-02 23:05:27,322 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2024-05-02 23:05:27,323 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2024-05-02 23:05:27,341 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-pig1.pig-49022241-9885-4fc4-ba9e-281674bb9e72\n",
      "2024-05-02 23:05:27,341 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n",
      "2024-05-02 23:05:27,615 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2024-05-02 23:05:27,619 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6000: <file ./scripts/pig1.pig, line 21, column 0> Output Location Validation Failed for: 'file:///workspace/output/high_value_transactions More info to follow:\n",
      "Output directory file:/workspace/output/high_value_transactions already exists\n",
      "Details at logfile: /workspace/pig_1714691126990.log\n",
      "2024-05-02 23:05:27,636 [main] INFO  org.apache.pig.Main - Pig script completed in 1 second and 18 milliseconds (1018 ms)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_pig_script(script_path, data_path):\n",
    "    # Set up the environment variable to point to the directory containing the data file\n",
    "    os.environ['PIG_DATA_PATH'] = data_path\n",
    "\n",
    "    # Execute the Pig script using subprocess, assuming Pig is installed and configured to run in local mode\n",
    "    result = subprocess.run(['pig', '-x', 'local', '-f', script_path], capture_output=True, text=True)\n",
    "    \n",
    "    # Check the results of the Pig script execution\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error occurred during Pig script execution:\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"Pig script executed successfully. Output:\")\n",
    "        print(result.stdout)\n",
    "\n",
    "# Define the path to the Pig script and the directory containing the data file\n",
    "pig_script = './scripts/pig1.pig'\n",
    "csv_data = './data/'\n",
    "\n",
    "# Execute the Pig script\n",
    "run_pig_script(pig_script, csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depStore</th>\n",
       "      <th>total_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>store1</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store2</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store3</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store3</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          depStore  total_sales\n",
       "store1  2022-01-02          300\n",
       "store2  2022-01-02          250\n",
       "store3  2022-01-01          400\n",
       "store3  2022-01-02          500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the output file created by Pig\n",
    "output_file = 'output/high_value_transactions/part-m-00000'  # Adjust path as needed\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(output_file, names=['depStore', 'total_sales'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LLM Layer - Pig2PySpark \n",
    "* https://medium.com/@yash9439/unleashing-the-power-of-falcon-code-a-comparative-analysis-of-implementation-approaches-803048ce65dc\n",
    "* ref: https://medium.com/@ajay_khanna/leveraging-llama2-0-for-question-answering-on-your-own-data-using-cpu-aa6f75868d2d\n",
    "* ref: https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476\n",
    "* https://wellsr.com/python/fine-tuning-llama2-for-question-answering-tasks/\n",
    "* https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-- Load the data from a CSV file\\ntransactions = LOAD 'data/sample1.csv' USING PigStorage(',') \\n    AS (depStore:chararray, date:chararray, amount:int);\\n\\n-- Filter transactions to include only those where the amount is greater than 200\\nhigh_value_transactions = FILTER transactions BY amount > 200;\\n\\n-- Group the transactions by store\\ngrouped_by_store = GROUP high_value_transactions BY depStore;\\n\\n-- Calculate total and average sales per depStore\\nsales_summary = FOREACH grouped_by_store GENERATE \\n    group AS depStore,\\n    SUM(high_value_transactions.amount) AS total_sales,\\n    AVG(high_value_transactions.amount) AS average_sales;\\n\\n-- Store the summary in a CSV file\\nSTORE sales_summary INTO 'output/sales_summary' USING PigStorage(',');\\n\\n-- Optional: Just for demonstration, store filtered data to another directory\\nSTORE high_value_transactions INTO 'output/high_value_transactions' USING PigStorage(',');\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards:   0%|          | 0/59 [00:09<?, ?it/s]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fedf181d1d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fedf181d1d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x22B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# messages = [\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     {\"role\": \"system\", \"content\": \"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark.\"},\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     {\"role\": \"user\", \"content\": f\"Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark. Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpig_script_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     23\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py:3436\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3435\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3436\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3445\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3447\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3448\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3449\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3452\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3453\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3454\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3455\u001b[0m ):\n\u001b[1;32m   3456\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:1038\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:535\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    533\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 934\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    937\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/urllib3/response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark.\"},\n",
    "#     {\"role\": \"user\", \"content\": f\"Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark. Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain as lc\n",
    "\n",
    "# Define the model ID and initialize the LLM\n",
    "model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "llm = lc.LLM.from_pretrained(\n",
    "    model_id=model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Initial prompt setup\n",
    "initial_prompt = \"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark. Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"\n",
    "\n",
    "# Define a function to execute PySpark code and capture any errors\n",
    "def execute_pyspark_code(code):\n",
    "    try:\n",
    "        # Assuming a function `execute_code` that runs PySpark code and returns results or errors\n",
    "        result = execute_code(code)\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "# Define a function to refine code using error messages\n",
    "def refine_code_with_error(code, error_message):\n",
    "    feedback_prompt = f\"{initial_prompt} However, the following error occurred when executing your previous PySpark code: {error_message} Please fix it. Previous PySpark code: {code}\"\n",
    "    messages = [{\"role\": \"user\", \"content\": feedback_prompt}]\n",
    "    result = llm_chain.run(input=messages)\n",
    "    return result.generated_text\n",
    "\n",
    "# Create a chain for generating PySpark code\n",
    "llm_chain = lc.Chain(\n",
    "    steps=[\n",
    "        lc.steps.TextGeneration(\n",
    "            llm=llm,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            end_tokens=[\"\", llm.tokenizer.eos_token]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initial code generation\n",
    "initial_messages = [{\"role\": \"user\", \"content\": initial_prompt}]\n",
    "initial_result = llm_chain.run(input=initial_messages)\n",
    "generated_code = initial_result.generated_text\n",
    "\n",
    "# Attempt to execute the generated code\n",
    "execution_result, error = execute_pyspark_code(generated_code)\n",
    "\n",
    "# If an error occurs, refine the code\n",
    "if error:\n",
    "    refined_code = refine_code_with_error(generated_code, error)\n",
    "    print(\"Refined PySpark Code:\", refined_code)\n",
    "else:\n",
    "    print(\"Successfully executed PySpark Code:\", execution_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# # model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"gradientai/Llama-3-8B-Instruct-Gradient-1048k\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "\n",
    "# # messages = [\n",
    "# #     {\"role\": \"system\", \"content\": \"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark.\"},\n",
    "# #     {\"role\": \"user\", \"content\": f\"Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\n",
    "# # ]\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": f\"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark. Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\n",
    "# ]\n",
    "\n",
    "# prompt = pipeline.tokenizer.apply_chat_template(\n",
    "#         messages, \n",
    "#         tokenize=False, \n",
    "#         add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# terminators = [\n",
    "#     pipeline.tokenizer.eos_token_id,\n",
    "#     pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# outputs = pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=256,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# # model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_id,\n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "\n",
    "# # messages = [\n",
    "# #     {\"role\": \"system\", \"content\": \"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark.\"},\n",
    "# #     {\"role\": \"user\", \"content\": f\"Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\n",
    "# # ]\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": f\"You are a highly experienced software engineer and machine learning engineer fluent in both PIG and PySpark. Re-write the following PIG code into PySpark code. Make sure to only share PySpark so that the two codes are logically identical and output identical results given input. PIG code: {pig_script_code}\"},\n",
    "# ]\n",
    "\n",
    "# prompt = pipeline.tokenizer.apply_chat_template(\n",
    "#         messages, \n",
    "#         tokenize=False, \n",
    "#         add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# terminators = [\n",
    "#     pipeline.tokenizer.eos_token_id,\n",
    "#     pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# outputs = pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=256,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# print(outputs[0][\"generated_text\"][len(prompt):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure! Below is a Python code template that uses an LLM to convert PIG code to PySpark code, then automatically tests and refines the PySpark code based on feedback. This example assumes that you are using an API to interact with an LLM and are capable of running PySpark scripts programmatically. Here, Ill outline a simplified version that focuses on the core loop and error handling.\n",
    "\n",
    "# ### Python Code Example\n",
    "\n",
    "# ```python\n",
    "# import requests\n",
    "\n",
    "# def get_pyspark_code(pig_code):\n",
    "#     \"\"\" Send PIG code to LLM and receive initial PySpark translation \"\"\"\n",
    "#     response = requests.post('https://api.example.com/translate', json={'code': pig_code})\n",
    "#     return response.json()['pyspark_code']\n",
    "\n",
    "# def test_pyspark_code(pyspark_code, test_input):s\n",
    "#     \"\"\"\n",
    "#     Run PySpark code and return the output.\n",
    "#     This function would realistically run the code in a separate environment and capture outputs.\n",
    "#     \"\"\"\n",
    "#     # Placeholder for actual code execution and output capturing\n",
    "#     # You would replace this with actual execution logic\n",
    "#     output = run_pyspark_script(pyspark_code, test_input)\n",
    "#     return output\n",
    "\n",
    "# def run_pyspark_script(pyspark_code, input_data):\n",
    "#     \"\"\" A mock function to execute PySpark code - replace with actual execution logic \"\"\"\n",
    "#     # This would involve setting up Spark session and running the code\n",
    "#     # For simplicity, this is just a placeholder\n",
    "#     return \"Mock output based on \" + input_data\n",
    "\n",
    "# def main(pig_code, test_input, expected_output):\n",
    "#     pyspark_code = get_pyspark_code(pig_code)\n",
    "#     attempts = 0\n",
    "#     max_attempts = 10  # Prevent infinite loops by limiting the number of attempts\n",
    "\n",
    "#     while attempts < max_attempts:\n",
    "#         attempts += 1\n",
    "#         output = test_pyspark_code(pyspark_code, test_input)\n",
    "#         if output == expected_output:\n",
    "#             print(\"Success! PySpark code matches the expected output.\")\n",
    "#             break\n",
    "#         else:\n",
    "#             print(\"Mismatch or error detected. Refining code...\")\n",
    "#             # Send feedback and get refined code\n",
    "#             response = requests.post('https://api.example.com/refine', json={\n",
    "#                 'original_code': pyspark_code,\n",
    "#                 'output': output,\n",
    "#                 'expected_output': expected_output\n",
    "#             })\n",
    "#             pyspark_code = response.json()['refined_pyspark_code']\n",
    "#     else:\n",
    "#         print(\"Maximum attempts reached. Final PySpark code may need manual review.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     pig_code = \"LOAD 'data' AS (a:int, b:int, c:int); FILTER a by a > 0; STORE a INTO 'output';\"\n",
    "#     test_input = \"Sample input data that your PIG code expects\"\n",
    "#     expected_output = \"Expected output format or data\"\n",
    "#     main(pig_code, test_input, expected_output)\n",
    "# ```\n",
    "\n",
    "# ### Key Components:\n",
    "\n",
    "# 1. **API Interaction**: This script assumes the existence of two endpoints: one to translate PIG to PySpark, and another to refine the PySpark code based on feedback.\n",
    "# 2. **Execution and Testing**: It simplistically represents the PySpark execution. In a real-world scenario, this would involve using a `subprocess` to run a Spark job, capturing its output, and perhaps even interacting with a test database or data storage.\n",
    "# 3. **Loop and Feedback**: The script uses a loop to refine the PySpark code based on mismatches between expected and actual outputs, up to a maximum number of iterations to prevent infinite loops.\n",
    "\n",
    "# ### Enhancements for Production:\n",
    "\n",
    "# - **Error Handling**: Implement robust error handling around API calls and PySpark execution.\n",
    "# - **Logging**: Add detailed logging for each step to facilitate debugging and monitoring.\n",
    "# - **Security**: Secure API interactions and handle data securely, especially when working with sensitive or large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================================\n",
    "# parse Python code inside the code cell (TODO: How to ensure code is consistently inside the triple back ticks?) \n",
    "# How to loop the code so that it runs until correct code is written? \n",
    "# How does Langchain come into play? \n",
    "# When debugging: 1) provide loaded data head 2) code 3) error message or output if run was successful --> output updated code\n",
    "# ==============================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# # Define the model\n",
    "# # model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# # Save model and tokenizer to a specific directory (optional)\n",
    "# # model.save_pretrained('./model_cache/falcon-7b-instruct')\n",
    "# # tokenizer.save_pretrained('./model_cache/falcon-7b-instruct')\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# # Download and cache model and tokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.save_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "# del model \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.save_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "# del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model and tokenizer from the cache for use\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "# model = AutoModelForCausalLM.from_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "# # Setup the pipeline with local model and tokenizer\n",
    "# text_generation = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0  # Assuming using the first GPU\n",
    "# )\n",
    "\n",
    "# # Generate text\n",
    "# generated_text = text_generation(\"Sample prompt text goes here\", max_length=50)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = f\"Re-write the following PIG code into PySpark code. Following is the PIG code: \\n {pig_script_code}\"\n",
    "# template = f\"\"\"\n",
    "# You are an intelligent software engineer and machine learning engineer. Re-write the following PIG code into PySpark code. Make sure to only share PySpark code so it's easy to copy and paste. \n",
    "# PIG code: {question}\n",
    "# --------------------------------------------------------------------\n",
    "# PySpark code:\"\"\"\n",
    "\n",
    "# sequences = pipeline(\n",
    "#     template,\n",
    "#     max_length=5000,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test PySpark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_starter(): \n",
    "    \"\"\"\n",
    "    Generate prompt to start transcribing PIG to PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_fix_error(): \n",
    "    \"\"\"\n",
    "    Generate prompt to fix error in PySpark code.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_fix_output(): \n",
    "    \"\"\"\n",
    "    Generate prompt to fix code output mismatch (between result from PIG and PySpark). \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "\n",
    "def run_pyspark_code(code_dir, code_name, sample_data_dir): \n",
    "    # start PySpark server\n",
    "    # run code against test data\n",
    "\n",
    "    if error: \n",
    "        return error_message\n",
    "    else: \n",
    "        return result_df\n",
    "\n",
    "def check_resutls(pig_result_df, pyspark_result_df): \n",
    "    # return True if the results are identical \n",
    "    return is_same \n",
    "\n",
    "\n",
    "# loop until both pyspark code runs fine and output data from PIG and PYSpark are the same\n",
    "while not is_pyspark_pass or not is_result_identical:  \n",
    "    \n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Sales Summary\").getOrCreate()\n",
    "\n",
    "# Load the data from a CSV file\n",
    "transactions = spark.read.csv('data/sample1.csv', header=False, inferSchema=True)\n",
    "\n",
    "# Define your list of column names\n",
    "column_names = [\"store\", \"date\", \"amount\"]\n",
    "\n",
    "# Filter transactions to include only those where the amount is greater than 200\n",
    "high_value_transactions = transactions.filter(transactions.amount > 200)\n",
    "\n",
    "# Group the transactions by store\n",
    "grouped_by_store = high_value_transactions.groupBy(\"depStore\")\n",
    "\n",
    "# Calculate total and average sales per depStore\n",
    "sales_summary = grouped_by_store.agg({\"amount\": \"sum\"}).alias(\"total_sales\"), avg(\"amount\").alias(\"average_sales\")\n",
    "\n",
    "# Store the summary in a CSV file\n",
    "sales_summary.write.csv('output/sales_summary', header=True, sep=',')\n",
    "\n",
    "# Optional: Just for demonstration, store filtered data to another directory\n",
    "high_value_transactions.write.csv('output/high_value_transactions', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from tabulate import tabulate\n",
    "\n",
    "def build_prompt_starter(pig_code):\n",
    "    \"\"\"\n",
    "    Generate prompt to start transcribing PIG to PySpark.\n",
    "    Returns a string with the starter prompt.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    I need to convert the following Apache PIG script into Apache PySpark code. \n",
    "    The PySpark code should perform the same tasks and produce identical outputs as the PIG code. \n",
    "    Please ensure that the PySpark code uses DataFrame operations wherever possible and include comments explaining any complex parts or transformations.\n",
    "    \n",
    "    PIG Code: \n",
    "    {pig_code}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def build_prompt_fix_error(error_message):\n",
    "    \"\"\"\n",
    "    Generate prompt to fix error in PySpark code.\n",
    "    Returns a string with the prompt to fix errors.\n",
    "    \"\"\"\n",
    "    return f\"\"\"There was an error in your PySpark code: {error_message}. Please fix it and share an updated PySpark code.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "pig_output = tabulate(pig_output, headers='keys', tablefmt='psql', showindex=\"never\")\n",
    "pyspark_output = tabulate(pyspark_output, headers='keys', tablefmt='psql', showindex=\"never\")\n",
    "\n",
    "\n",
    "def build_prompt_fix_output(pig_output, pyspark_output):\n",
    "    \"\"\"\n",
    "    Generate prompt to fix code output mismatch between results from PIG and PySpark.\n",
    "    Returns a string with the prompt for fixing output mismatches.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    The outputs between PIG and PySpark do not match. Please adjust the PySpark code so that the output from PySpark code is identical to that from PIG code. \n",
    "    Below are the two outputs with mismatch:\n",
    "\n",
    "    Pig Output (ground truth): \n",
    "    {pig_output}\n",
    "\n",
    "    \n",
    "    PySpark Output: \n",
    "    {pyspark_output}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def run_pyspark_code(code):\n",
    "    \"\"\"\n",
    "    Executes PySpark code, returns either error message or result DataFrame.\n",
    "    Assumes PySpark session and context are already set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exec(code)\n",
    "        return None, globals().get('sales_summary')  # Assuming 'sales_summary' is the result DataFrame\n",
    "    except Exception as e:\n",
    "        return str(e), None\n",
    "\n",
    "\n",
    "def check_results(pig_result_df, pyspark_result_df):\n",
    "    \"\"\"\n",
    "    Compare PIG and PySpark DataFrames to check if the results are identical.\n",
    "    \"\"\"\n",
    "    return pig_result_df.subtract(pyspark_result_df).count() == 0 and pyspark_result_df.subtract(pig_result_df).count() == 0\n",
    "\n",
    "\n",
    "# Set up PySpark\n",
    "spark = SparkSession.builder.appName(\"Sales Summary\").getOrCreate()\n",
    "\n",
    "# Initial PySpark code generation using an LLM (not shown here)\n",
    "pyspark_code = \"\"\"\n",
    "# Assume pyspark_code is filled with the initially generated code\n",
    "\"\"\"\n",
    "pig_result_df = spark.createDataFrame(...)  # Assume this is setup elsewhere\n",
    "\n",
    "# Run and refine PySpark code\n",
    "error_message, pyspark_result_df = run_pyspark_code(pyspark_code)\n",
    "while error_message or not check_results(pig_result_df, pyspark_result_df):\n",
    "    if error_message:\n",
    "        prompt = build_prompt_fix_error(error_message)\n",
    "    else:\n",
    "        prompt = build_prompt_fix_output()\n",
    "    # Here you would update `pyspark_code` based on the LLM's output (not shown here)\n",
    "    error_message, pyspark_result_df = run_pyspark_code(pyspark_code)\n",
    "\n",
    "# Results are now fine\n",
    "print(\"PySpark code executed successfully and results match PIG.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess to test out sample codes: \n",
    "# link: https://www.google.com/search?q=Python+application+which+run+PIG+code&rlz=1C1OPNX_enUS1108US1108&oq=Python+application+which+run+PIG+code+&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRigAdIBCTEwNTIzajBqN6gCALACAA&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "# langchain \n",
    "\n",
    "# langsmith \n",
    "\n",
    "# streamlit \n",
    "\n",
    "# airflow \n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
