{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Transcribe \n",
    "* Langchain\n",
    "* LangSmith\n",
    "* Semantic Kernel\n",
    "* Async Calls\n",
    "* Build APIs\n",
    "* Integrate Airflow, DB, Logging\n",
    "\n",
    "Models:\n",
    "* Mistral Dolphin: \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take user input. User can either paste or upload PIG code. The uploaded or pasted file will be saved to a local directory (or a database) \n",
    "2. (optional) user uploads a sample data to run against the PIG code. THe output from the code will be used to validate PySpark code by ensuring the two outputs are identical. \n",
    "3. If no sample data is uploaded, LLM generates a sample data given the PIG code. \n",
    "3.1. LLM generate Python code that build and save sample file to local directory and database\n",
    "3.2. PIG executor runs the PIG code to 1) ensure the sample data is runnable and 2) save the output(s) for future comparison with results from PySpark code \n",
    "4. LLM generates PySpark that is equivalent to PIG code. \n",
    "5. PySpark executor runs the PySpark code. \n",
    "5.1. If error is occured, the error is fed back to LLM to fix the PySPark code \n",
    "5.2. If no error occured \n",
    "5.2.1. If output data is different from the one resulting from PIG code, the difference is fed back to LLM to fix the PySpark code \n",
    "5.2.2. If output data is identical to the one resulting from PIG code, then we can tell that the correct PYSpark code that is identical to PIG code is written> \n",
    "6. PySpark code is returned to user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Use LangGraph \n",
    "# TODO 2: Use Ollama to run LLM models locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be handled in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "import regex as re\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pyspark as spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 3/3 [05:59<00:00, 119.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 1min 57s, total: 3min 18s\n",
      "Wall time: 6min 40s\n"
     ]
    }
   ],
   "source": [
    "class LanguageModelService:\n",
    "    def __init__(self, model_id=\"mistralai/Mistral-7B-Instruct-v0.2\", # LLM pre-trained model \n",
    "                 device=0, # Use 1st GPU \n",
    "                 max_new_tokens=1000 # output token count \n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the model with the specified parameters.\n",
    "        \"\"\"\n",
    "        self.hf = HuggingFacePipeline.from_model_id(\n",
    "            model_id=model_id,\n",
    "            task=\"text-generation\",\n",
    "            device=device,\n",
    "            pipeline_kwargs={\"max_new_tokens\": max_new_tokens},\n",
    "        )\n",
    "        \n",
    "    def query(self, input_text):\n",
    "        \"\"\"\n",
    "        Sends a custom query to the model and returns the output.\n",
    "        \"\"\"\n",
    "        result = self.hf(input_text)\n",
    "        return result.text if hasattr(result, 'text') else result\n",
    "\n",
    "# Usage\n",
    "%time  lm_service = LanguageModelService()  # Initialize once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load PIG Code (optionsal: Upload sample data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Load the data from a CSV file\n",
      "transactions = LOAD 'data/sample1.csv' USING PigStorage(',') \n",
      "    AS (depStore:chararray, date:chararray, amount:int);\n",
      "\n",
      "-- Filter transactions to include only those where the amount is greater than 200\n",
      "high_value_transactions = FILTER transactions BY amount > 200;\n",
      "\n",
      "-- Group the transactions by store\n",
      "grouped_by_store = GROUP high_value_transactions BY depStore;\n",
      "\n",
      "-- Calculate total and average sales per depStore\n",
      "sales_summary = FOREACH grouped_by_store GENERATE \n",
      "    group AS depStore,\n",
      "    SUM(high_value_transactions.amount) AS total_sales,\n",
      "    AVG(high_value_transactions.amount) AS average_sales;\n",
      "\n",
      "-- Store the summary in a CSV file\n",
      "STORE sales_summary INTO 'output/sales_summary' USING PigStorage(',');\n",
      "\n",
      "-- Optional: Just for demonstration, store filtered data to another directory\n",
      "STORE high_value_transactions INTO 'output/high_value_transactions' USING PigStorage(',');\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temporarily work with pre-written code\n",
    "pig_script_dir = './scripts/pig1.pig'\n",
    "\n",
    "with open(pig_script_dir, 'r') as file:\n",
    "    pig_code = file.read()\n",
    "\n",
    "print(pig_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create sample data \n",
    "\n",
    "Note Batch GPU can work: https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create sample test data using LLM (if none provided by user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from model:\n",
      " \n",
      "```python\n",
      "import csv\n",
      "\n",
      "data = [\n",
      "    ['StoreA', '2022-01-01', 250],\n",
      "    ['StoreB', '2022-01-02', 300],\n",
      "    ['StoreA', '2022-01-03', 220],\n",
      "    ['StoreC', '2022-01-04', 280],\n",
      "    ['StoreB', '2022-01-05', 210],\n",
      "    ['StoreA', '2022-01-06', 350],\n",
      "    ['StoreC', '2022-01-07', 290],\n",
      "    ['StoreB', '2022-01-08', 260],\n",
      "    ['StoreA', '2022-01-09', 270],\n",
      "    ['StoreC', '2022-01-10', 310],\n",
      "]\n",
      "\n",
      "with open('data/sample1.csv', 'w', newline='') as csvfile:\n",
      "    writer = csv.writer(csvfile, delimiter=',')\n",
      "    writer.writerow([\"depStore\", \"date\", \"amount\"])\n",
      "    writer.writerows(data)\n",
      "```\n",
      "\n",
      "This Python code creates a list of transactions and writes it to a CSV file named 'data/sample1.csv' with a header row. The data is diverse enough to test the filtering, grouping, and aggregation operations in the PIG code effectively.\n",
      "CPU times: user 7min 12s, sys: 47.2 s, total: 8min\n",
      "Wall time: 8min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "query_create_sample_data = f\"\"\"\n",
    "    Given the following PIG code, generate sample CSV data that will produce consistent \n",
    "    results when processed by this PIG code. The PIG code is intended to perform operations \n",
    "    such as filtering, grouping, and aggregation. Ensure that the sample data is diverse enough \n",
    "    to test all parts of the code effectively.\\n\\n\n",
    "    PIG Code:\\n{pig_code}\\n\\n\n",
    "    Write a Python code that will save the sample CSV file to ./data/sample1.csv with a header column. Encapsulate the code between ```. \n",
    "    Make sure there is only one code chunk (```).:\n",
    "\"\"\"\n",
    "\n",
    "response = lm_service.query(query_create_sample_data)\n",
    "print(\"Response from model:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_indentation(code):\n",
    "    lines = code.split('\\n')\n",
    "    # Find the first non-empty line to determine the base indentation level\n",
    "    base_indent = None\n",
    "    for line in lines:\n",
    "        stripped_line = line.lstrip()\n",
    "        if stripped_line:\n",
    "            base_indent = len(line) - len(stripped_line)\n",
    "            break\n",
    "\n",
    "    if base_indent is None:\n",
    "        return code  # Return original code if it's all empty lines or no base indent found\n",
    "\n",
    "    # Normalize each line by removing the base indentation\n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        stripped_line = line.lstrip()\n",
    "        if len(line) > base_indent:\n",
    "            normalized_lines.append(line[base_indent:])\n",
    "        else:\n",
    "            normalized_lines.append(stripped_line)\n",
    "\n",
    "    return '\\n'.join(normalized_lines)\n",
    "\n",
    "def parse_python_code_from_text(text):\n",
    "    normalized_text = normalize_indentation(text)\n",
    "    \n",
    "    # Define the pattern to extract code between ```python and ```\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    match = re.search(pattern, normalized_text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        code_to_execute = match.group(1)\n",
    "        print(code_to_execute)\n",
    "        return code_to_execute\n",
    "    else:\n",
    "        print(\"No Python code block found.\")\n",
    "\n",
    "def run_pyspark_code(code):\n",
    "    \"\"\"\n",
    "    Executes PySpark code, returns either error message or result DataFrame.\n",
    "    Assumes PySpark session and context are already set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exec(code)\n",
    "        return None, globals().get('sales_summary')  # Assuming 'sales_summary' is the result DataFrame\n",
    "    except Exception as e:\n",
    "        return str(e), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2. LLM may not output working test data. Repeat until correct data is outputtedz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Save sample data and output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. Save Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import csv\n",
      "\n",
      "data = [\n",
      "    ['StoreA', '2022-01-01', 250],\n",
      "    ['StoreB', '2022-01-02', 300],\n",
      "    ['StoreA', '2022-01-03', 220],\n",
      "    ['StoreC', '2022-01-04', 280],\n",
      "    ['StoreB', '2022-01-05', 210],\n",
      "    ['StoreA', '2022-01-06', 350],\n",
      "    ['StoreC', '2022-01-07', 290],\n",
      "    ['StoreB', '2022-01-08', 260],\n",
      "    ['StoreA', '2022-01-09', 270],\n",
      "    ['StoreC', '2022-01-10', 310],\n",
      "]\n",
      "\n",
      "with open('data/sample1.csv', 'w', newline='') as csvfile:\n",
      "    writer = csv.writer(csvfile, delimiter=',')\n",
      "    writer.writerow([\"depStore\", \"date\", \"amount\"])\n",
      "    writer.writerows(data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = parse_python_code_from_text(response)\n",
    "run_pyspark_code(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Execute PIG code and save output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred during Pig script execution:\n",
      "2024-05-06 03:03:50,063 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n",
      "2024-05-06 03:03:50,063 [main] INFO  org.apache.pig.Main - Logging error messages to: /workspace/pig_1714964630057.log\n",
      "2024-05-06 03:03:50,079 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2024-05-06 03:03:50,301 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2024-05-06 03:03:50,366 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2024-05-06 03:03:50,367 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2024-05-06 03:03:50,382 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-pig1.pig-56c51554-a370-4bef-8174-675e5e4d52f8\n",
      "2024-05-06 03:03:50,383 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n",
      "2024-05-06 03:03:50,662 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2024-05-06 03:03:50,665 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 6000: <file ./scripts/pig1.pig, line 21, column 0> Output Location Validation Failed for: 'file:///workspace/output/high_value_transactions More info to follow:\n",
      "Output directory file:/workspace/output/high_value_transactions already exists\n",
      "Details at logfile: /workspace/pig_1714964630057.log\n",
      "2024-05-06 03:03:50,682 [main] INFO  org.apache.pig.Main - Pig script completed in 997 milliseconds (997 ms)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_pig_script(script_path, data_path):\n",
    "    # Set up the environment variable to point to the directory containing the data file\n",
    "    os.environ['PIG_DATA_PATH'] = data_path\n",
    "\n",
    "    # Execute the Pig script using subprocess, assuming Pig is installed and configured to run in local mode\n",
    "    result = subprocess.run(['pig', '-x', 'local', '-f', script_path], capture_output=True, text=True)\n",
    "    \n",
    "    # Check the results of the Pig script execution\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error occurred during Pig script execution:\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"Pig script executed successfully. Output:\")\n",
    "        print(result.stdout)\n",
    "\n",
    "# Define the path to the Pig script and the directory containing the data file\n",
    "pig_script = './scripts/pig1.pig'\n",
    "csv_data = './data/'\n",
    "\n",
    "# Execute the Pig script\n",
    "run_pig_script(pig_script, csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest file is: ./output/high_value_transactions/part-00000-e24ceab2-7948-433d-b84d-e7078314d13f-c000.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_latest_file(directory, pattern):\n",
    "    \"\"\"\n",
    "    Returns the path to the latest file in the given directory that matches the pattern.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search in.\n",
    "    pattern (str): The file name pattern to match.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the latest file matching the pattern or None if no file matches.\n",
    "    \"\"\"\n",
    "    # Create the full path pattern\n",
    "    search_pattern = os.path.join(directory, pattern)\n",
    "    \n",
    "    # List all files matching the pattern\n",
    "    files = glob.glob(search_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        return None\n",
    "\n",
    "    # Find the latest file based on last modification time\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    \n",
    "    return latest_file\n",
    "\n",
    "# Example usage\n",
    "directory = './output/high_value_transactions'\n",
    "file_pattern = 'part-*'\n",
    "latest_file = get_latest_file(directory, file_pattern)\n",
    "print(f\"The latest file is: {latest_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depStore</th>\n",
       "      <th>total_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>depStore</th>\n",
       "      <td>date</td>\n",
       "      <td>amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoreA</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoreB</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoreA</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StoreC</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            depStore total_sales\n",
       "depStore        date      amount\n",
       "StoreA    2022-01-01         250\n",
       "StoreB    2022-01-02         300\n",
       "StoreA    2022-01-03         220\n",
       "StoreC    2022-01-04         280"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the output file created by Pig\n",
    "output_file = latest_file  # Adjust path as needed\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(output_file, names=['depStore', 'total_sales'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.1. LLM transcriber PIG2PySpark\n",
    "# Prompt: \n",
    "# You are an experienced Software Engineer and Machine Learning Engineer fluent in PIG and PySpark coding languages. \n",
    "# Rewrite the following PIG code into PySpark so that they can perform identical tasks and output identical results given same input data. \n",
    "# PIG Code: {pig_code}\n",
    "# A.2. Code Parser: Parse and save PySpark code \n",
    "\n",
    "# B. Sample Data Builder \n",
    "# C. PIG Interpreter \n",
    "# D. PySpark Interpreter \n",
    "\n",
    "# If sample data NOT available (run B.)\n",
    "  # Run PIG code and generate output \n",
    "  # 1. Generate PySpark code from PIG (run A.1) and parse/save the PySpark Code (run A.2)\n",
    "# 2. Run against sample data in a separate module.\n",
    "# 3. Check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LLM Layer - Pig2PySpark \n",
    "* https://medium.com/@yash9439/unleashing-the-power-of-falcon-code-a-comparative-analysis-of-implementation-approaches-803048ce65dc\n",
    "* ref: https://medium.com/@ajay_khanna/leveraging-llama2-0-for-question-answering-on-your-own-data-using-cpu-aa6f75868d2d\n",
    "* ref: https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476\n",
    "* https://wellsr.com/python/fine-tuning-llama2-for-question-answering-tasks/\n",
    "* https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 21.3 s, total: 3min 24s\n",
      "Wall time: 3min 24s\n",
      "Response from model:\n",
      " ```\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession, functions as F\n",
      "\n",
      "# Load the data from a CSV file\n",
      "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
      "transactions = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/sample1.csv\")\n",
      "\n",
      "# Filter transactions to include only those where the amount is greater than 200\n",
      "high_value_transactions = transactions.filter(F.col(\"amount\") > 200)\n",
      "\n",
      "# Group the transactions by store\n",
      "grouped_by_store = high_value_transactions.groupBy(\"depStore\")\n",
      "\n",
      "# Calculate total and average sales per depStore\n",
      "sales_summary = grouped_by_store.agg(F.sum(\"amount\").alias(\"total_sales\"), F.avg(\"amount\").alias(\"average_sales\"))\n",
      "\n",
      "# Store the summary in a CSV file\n",
      "sales_summary.write.option(\"header\", \"true\").csv(\"output/sales_summary\", mode=\"overwrite\")\n",
      "\n",
      "# Optional: Just for demonstration, store filtered data to another directory\n",
      "high_value_transactions.write.option(\"header\", \"true\").csv(\"output/high_value_transactions\", mode=\"overwrite\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_pig2pyspark = f\"\"\"\n",
    "You are an experienced software and machine learning engineer fluent in both PIG and PySpark. \n",
    "Re-write the following PIG code into PySpark code. \n",
    "Ensure PySpark code is logically identical and output identical results as the provided PIG code. \n",
    "Make sure to only share PySpark in a single code block (inside ```). \n",
    "PIG code: {pig_code}\n",
    "\"\"\"\n",
    "\n",
    "%time response = lm_service.query(prompt_pig2pyspark)\n",
    "print(\"Response from model:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('invalid syntax (<string>, line 1)', None)\n"
     ]
    }
   ],
   "source": [
    "error_message = run_pyspark_code(response)\n",
    "print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_fix_error(pyspark_code, error_message, input_data):\n",
    "    \"\"\"\n",
    "    Generate prompt to fix error in PySpark code.\n",
    "    Returns a string with the prompt to fix errors.\n",
    "    \"\"\"\n",
    "    return f\"\"\"There was an error in your PySpark code: {error_message}. Please fix and re-share the full PySpark code with relevant updates inside a single code cell.\n",
    "    \n",
    "    Below is the PySpark code that returned an error: \n",
    "    {pyspark_code}. \n",
    "\n",
    "    Below is the input data: \n",
    "    {input_data}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from model:\n",
      " \n",
      "    Below is the expected output: \n",
      "                depStore  total_sales  average_sales\n",
      "StoreA            3150         315.0\n",
      "StoreB            1120         112.0\n",
      "StoreC            1180         118.0\n",
      "```\n",
      "\n",
      "To fix the error, you need to remove the extra parentheses in the `agg` function. Here's the corrected code:\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession, functions as F\n",
      "\n",
      "# Load the data from a CSV file\n",
      "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
      "transactions = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/sample1.csv\")\n",
      "\n",
      "# Filter transactions to include only those where the amount is greater than 200\n",
      "high_value_transactions = transactions.filter(F.col(\"amount\") > 200)\n",
      "\n",
      "# Group the transactions by store\n",
      "grouped_by_store = high_value_transactions.groupBy(\"depStore\")\n",
      "\n",
      "# Calculate total and average sales per depStore\n",
      "sales_summary = grouped_by_store.agg(F.sum(\"amount\").alias(\"total_sales\"), F.avg(\"amount\").alias(\"average_sales\"))\n",
      "\n",
      "# Store the summary in a CSV file\n",
      "sales_summary.write.option(\"header\", \"true\").csv(\"output/sales_summary\", mode=\"overwrite\")\n",
      "\n",
      "# Optional: Just for demonstration, store filtered data to another directory\n",
      "high_value_transactions.write.option(\"header\", \"true\").csv(\"output/high_value_transactions\", mode=\"overwrite\")\n",
      "```\n",
      "\n",
      "This corrected code should now run without errors and produce the expected output.\n"
     ]
    }
   ],
   "source": [
    "prompt_fix_pyspark_code = build_prompt_fix_error(response, error_message, df)\n",
    "\n",
    "response = lm_service.query(prompt_fix_pyspark_code)\n",
    "print(\"Response from model:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Below is the expected output: \n",
      "                depStore  total_sales  average_sales\n",
      "StoreA            3150         315.0\n",
      "StoreB            1120         112.0\n",
      "StoreC            1180         118.0\n",
      "```\n",
      "\n",
      "To fix the error, you need to remove the extra parentheses in the `agg` function. Here's the corrected code:\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession, functions as F\n",
      "\n",
      "# Load the data from a CSV file\n",
      "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
      "transactions = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/sample1.csv\")\n",
      "\n",
      "# Filter transactions to include only those where the amount is greater than 200\n",
      "high_value_transactions = transactions.filter(F.col(\"amount\") > 200)\n",
      "\n",
      "# Group the transactions by store\n",
      "grouped_by_store = high_value_transactions.groupBy(\"depStore\")\n",
      "\n",
      "# Calculate total and average sales per depStore\n",
      "sales_summary = grouped_by_store.agg(F.sum(\"amount\").alias(\"total_sales\"), F.avg(\"amount\").alias(\"average_sales\"))\n",
      "\n",
      "# Store the summary in a CSV file\n",
      "sales_summary.write.option(\"header\", \"true\").csv(\"output/sales_summary\", mode=\"overwrite\")\n",
      "\n",
      "# Optional: Just for demonstration, store filtered data to another directory\n",
      "high_value_transactions.write.option(\"header\", \"true\").csv(\"output/high_value_transactions\", mode=\"overwrite\")\n",
      "```\n",
      "\n",
      "This corrected code should now run without errors and produce the expected output.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Python code block found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/06 03:27:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = parse_python_code_from_text(response) # this did not run \n",
    "run_pyspark_code(\"\"\"from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Load the data from a CSV file\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n",
    "transactions = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/sample1.csv\")\n",
    "\n",
    "# Filter transactions to include only those where the amount is greater than 200\n",
    "high_value_transactions = transactions.filter(F.col(\"amount\") > 200)\n",
    "\n",
    "# Group the transactions by store\n",
    "grouped_by_store = high_value_transactions.groupBy(\"depStore\")\n",
    "\n",
    "# Calculate total and average sales per depStore\n",
    "sales_summary = grouped_by_store.agg(F.sum(\"amount\").alias(\"total_sales\"), F.avg(\"amount\").alias(\"average_sales\"))\n",
    "\n",
    "# Store the summary in a CSV file\n",
    "sales_summary.write.option(\"header\", \"true\").csv(\"output/sales_summary\", mode=\"overwrite\")\n",
    "\n",
    "# Optional: Just for demonstration, store filtered data to another directory\n",
    "high_value_transactions.write.option(\"header\", \"true\").csv(\"output/high_value_transactions\", mode=\"overwrite\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_pig_script(script_path, data_path):\n",
    "    # Set up the environment variable to point to the directory containing the data file\n",
    "    os.environ['PIG_DATA_PATH'] = data_path\n",
    "\n",
    "    # Execute the Pig script using subprocess, assuming Pig is installed and configured to run in local mode\n",
    "    result = subprocess.run(['pig', '-x', 'local', '-f', script_path], capture_output=True, text=True)\n",
    "    \n",
    "    # Check the results of the Pig script execution\n",
    "    if result.returncode != 0:\n",
    "        print(\"Error occurred during Pig script execution:\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"Pig script executed successfully. Output:\")\n",
    "        print(result.stdout)\n",
    "\n",
    "# Define the path to the Pig script and the directory containing the data file\n",
    "pig_script = './scripts/pig1.pig'\n",
    "csv_data = './data/'\n",
    "\n",
    "# Execute the Pig script\n",
    "run_pig_script(pig_script, csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_python_code_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pyspark_code(code):\n",
    "    \"\"\"\n",
    "    Executes PySpark code, returns either error message or result DataFrame.\n",
    "    Assumes PySpark session and context are already set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exec(code)\n",
    "        return None, globals().get('sales_summary')  # Assuming 'sales_summary' is the result DataFrame\n",
    "    except Exception as e:\n",
    "        return str(e), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================================\n",
    "# parse Python code inside the code cell (TODO: How to ensure code is consistently inside the triple back ticks?) \n",
    "# How to loop the code so that it runs until correct code is written? \n",
    "# How does Langchain come into play? \n",
    "# When debugging: 1) provide loaded data head 2) code 3) error message or output if run was successful --> output updated code\n",
    "# ==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model and tokenizer from the cache for use\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "# model = AutoModelForCausalLM.from_pretrained('./model_cache/Meta-Llama-3-8B-Instruct')\n",
    "\n",
    "# # Setup the pipeline with local model and tokenizer\n",
    "# text_generation = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0  # Assuming using the first GPU\n",
    "# )\n",
    "\n",
    "# # Generate text\n",
    "# generated_text = text_generation(\"Sample prompt text goes here\", max_length=50)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = f\"Re-write the following PIG code into PySpark code. Following is the PIG code: \\n {pig_script_code}\"\n",
    "# template = f\"\"\"\n",
    "# You are an intelligent software engineer and machine learning engineer. Re-write the following PIG code into PySpark code. Make sure to only share PySpark code so it's easy to copy and paste. \n",
    "# PIG code: {question}\n",
    "# --------------------------------------------------------------------\n",
    "# PySpark code:\"\"\"\n",
    "\n",
    "# sequences = pipeline(\n",
    "#     template,\n",
    "#     max_length=5000,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test PySpark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_starter(): \n",
    "    \"\"\"\n",
    "    Generate prompt to start transcribing PIG to PySpark.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_fix_error(): \n",
    "    \"\"\"\n",
    "    Generate prompt to fix error in PySpark code.\n",
    "    \"\"\"\n",
    "\n",
    "def build_prompt_fix_output(): \n",
    "    \"\"\"\n",
    "    Generate prompt to fix code output mismatch (between result from PIG and PySpark). \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "\n",
    "def run_pyspark_code(code_dir, code_name, sample_data_dir): \n",
    "    # start PySpark server\n",
    "    # run code against test data\n",
    "\n",
    "    if error: \n",
    "        return error_message\n",
    "    else: \n",
    "        return result_df\n",
    "\n",
    "def check_resutls(pig_result_df, pyspark_result_df): \n",
    "    # return True if the results are identical \n",
    "    return is_same \n",
    "\n",
    "\n",
    "# loop until both pyspark code runs fine and output data from PIG and PYSpark are the same\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from tabulate import tabulate\n",
    "\n",
    "def build_prompt_starter(pig_code):\n",
    "    \"\"\"\n",
    "    Generate prompt to start transcribing PIG to PySpark.\n",
    "    Returns a string with the starter prompt.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    I need to convert the following Apache PIG script into Apache PySpark code. \n",
    "    The PySpark code should perform the same tasks and produce identical outputs as the PIG code. \n",
    "    Please ensure that the PySpark code uses DataFrame operations wherever possible and include comments explaining any complex parts or transformations.\n",
    "    \n",
    "    PIG Code: \n",
    "    {pig_code}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pig_output = tabulate(pig_output, headers='keys', tablefmt='psql', showindex=\"never\")\n",
    "pyspark_output = tabulate(pyspark_output, headers='keys', tablefmt='psql', showindex=\"never\")\n",
    "\n",
    "\n",
    "def build_prompt_fix_output(pig_output, pyspark_output):\n",
    "    \"\"\"\n",
    "    Generate prompt to fix code output mismatch between results from PIG and PySpark.\n",
    "    Returns a string with the prompt for fixing output mismatches.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "    The outputs between PIG and PySpark do not match. Please adjust the PySpark code so that the output from PySpark code is identical to that from PIG code. \n",
    "    Below are the two outputs with mismatch:\n",
    "\n",
    "    Pig Output (ground truth): \n",
    "    {pig_output}\n",
    "\n",
    "    \n",
    "    PySpark Output: \n",
    "    {pyspark_output}\n",
    "    \"\"\"\n",
    "\n",
    "def check_results(pig_result_df, pyspark_result_df):\n",
    "    \"\"\"\n",
    "    Compare PIG and PySpark DataFrames to check if the results are identical.\n",
    "    \"\"\"\n",
    "    return pig_result_df.subtract(pyspark_result_df).count() == 0 and pyspark_result_df.subtract(pig_result_df).count() == 0\n",
    "\n",
    "\n",
    "# Set up PySpark\n",
    "spark = SparkSession.builder.appName(\"Sales Summary\").getOrCreate()\n",
    "\n",
    "# Initial PySpark code generation using an LLM (not shown here)\n",
    "pyspark_code = \"\"\"\n",
    "# Assume pyspark_code is filled with the initially generated code\n",
    "\"\"\"\n",
    "pig_result_df = spark.createDataFrame(...)  # Assume this is setup elsewhere\n",
    "\n",
    "# Run and refine PySpark code\n",
    "error_message, pyspark_result_df = run_pyspark_code(pyspark_code)\n",
    "while error_message or not check_results(pig_result_df, pyspark_result_df):\n",
    "    if error_message:\n",
    "        prompt = build_prompt_fix_error(error_message)\n",
    "    else:\n",
    "        prompt = build_prompt_fix_output()\n",
    "    # Here you would update `pyspark_code` based on the LLM's output (not shown here)\n",
    "    error_message, pyspark_result_df = run_pyspark_code(pyspark_code)\n",
    "\n",
    "# Results are now fine\n",
    "print(\"PySpark code executed successfully and results match PIG.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess to test out sample codes: \n",
    "# link: https://www.google.com/search?q=Python+application+which+run+PIG+code&rlz=1C1OPNX_enUS1108US1108&oq=Python+application+which+run+PIG+code+&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRigAdIBCTEwNTIzajBqN6gCALACAA&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "# langchain \n",
    "\n",
    "# langsmith \n",
    "\n",
    "# streamlit \n",
    "\n",
    "# airflow \n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
